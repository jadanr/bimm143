---
title: "Class 7 Machine Learning 1"
author: "Jada Ruiz"
format: gfm
---

# K-means Clustering 

First we will test how this method works in R with some made up data. 

```{r}
x <- rnorm(10000)
hist(x)
```

Let's make some numbers centered upon -3. 

```{r}
tmp <- c(rnorm(30, -3), rnorm(30, +3))

x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```

Now lets see how `kmeans()` works with this data...

```{r}
km <- kmeans(x, center=2, nstart=20)
km
```

```{r}
km$center
```

>Q. How many points are in each cluster?

```{r}
km$size
```

>Q. What 'components' of your result object details 
    - cluster assignment/membership?
    - cluster center?
    
```{r}
km$cluster
km$centers
```

>Q. Plot x colored by the kmeans cluster assignment and add cluster centers as blue points.

```{r}
plot(x, col=km$cluster)
points(km$centers, col="blue", pch=15, cex=2)
```

# Hierarchial Clustering 

The `hclust()` function in R performs hierarchical clustering. 

the `hclust` function requires an input distance matrix, which I can get from the `dist()` fucntion.

```{r}
hc <- hclust(dist(x))
hc
```

There is a plot() method for hclust objects…

```{r}
plot(hc)
```

Now to get my cluster membership vector I need to "cut" the tree to yield separate "branches" with the "leaves" on each branch being out cluster. To do this we use the `cutree()` function. 

```{r}
cutree(hc, h=8)
```

Use `cutree()` with a k=2.
```{r}
grps <- cutree(hc, k=2)
```

A plot of our data colored by our hclust grps.

```{r}
plot(x, col=grps)
```

# Principal Component Analysis (PCA)

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
x
```

>Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
dim(x)
```

There are 17 rows and 4 columns in the data frame, and you can use dim() to figure this out. 

Preview first 6 rows:
```{r}
head(x)
```

We want to fix columns, as there should be 5, make row names the first column.

```{r}
rownames(x) <- x[,1]
x <- x[,-1]
head(x)
```

Lets check number of rows and columns again.

```{r}
dim(x)
```

>Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

I prefer the second approach more because it is directly deriving from the original data frame and not from x which gets rewritten every time you run the code "x <- x[,-1]" multiple times. You'll lose the first column every time you rerun that code. 

Now, lets look at differences and trends of graphs.

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

>Q3: Changing what optional argument in the above barplot() function results in the following plot?

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```
Changing the beside to "F" (false) results in the type of plot above. 

>Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

```{r}
pairs(x, col=rainbow(10), pch=16)
```
This type of plot above compares each region with one another and the categories given in the data frame. If a point falls on a diagonal line that means the values between each region within that category is quite similar to one another. 

While this is kind of useful it takes work to dig into the details here to find out what is different in these countries.

>Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

N. Ireland typically has lower values for these categories compared to the other countries. Meaning that on the plots, N. Ireland has more outliers compared to other countries.

## PCA to the rescue

Principal Component Analysis (PCA for short) can be a big help in these cases where we have lot's of things that are being measured in a data set. 

The main PCA function in base R is 'prcomp()'.

The 'prcomp()' function wants aw input the transpose of our matrix/table/data.frame

```{r}
pca <- prcomp(t(x))
summary(pca)
```

The above results shows that PCA captures 67% of the total variance in the original data in one PC and 96.5% in two PCs. 

```{r}
attributes(pca)
```

```{r}
pca$x
```

>Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.
>Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.

Lets plot our main results.

```{r}
plot(pca$x[,1], pca$x[,2], col=c("orange", "red", "blue", "darkgreen"))
```

>Q9: Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about?

```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,2], las=2 )
```

Soft drinks and fresh potatoes are the most predominant categories. This mainly tells us that one country consumes more fresh potatoes while the other country consumes more soft drinks. 

# 2. PCAof RNA-seq Data
```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```

>Q10: How many genes and samples are in this data set?

```{r}
dim(rna.data)
```

There are 100 genes and 10 samples. 

Now PCA:

```{r}
pca <- prcomp(t(rna.data), scale=TRUE)
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2")
```

